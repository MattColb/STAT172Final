<<<<<<< Updated upstream
summary(cps_data)
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Iowa Residents on Food Stamps or SNAP",
fill = "Proportion of\nResidents")
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Iowa Residents on Food Stamps or SNAP",
fill = "Proportion of\nResidents")
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
acs_test_data <- model.matrix(~hhsize + married + education + elderly +
kids + black + hispanic + female, data=acs_data)[,-1]
fsstmp_predictions <- predict(lr_lasso_fsstmp, acs_test_data, type="response")[,1]
acs_predicted <- acs_data %>% mutate(
fsstmp_prediction = ifelse(fsstmp_predictions > lasso_fsstmp_pi_star, "On Assistance", "Not On Assistance")
)
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Iowa Residents on Food Stamps or SNAP",
fill = "Proportion of\nResidents")
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size,
has_senior = ifelse(elderly > 0, "Has Senior", "No Senior")
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Iowa Residents on Food Stamps or SNAP",
fill = "Proportion of\nResidents")
summary_by_PUMA
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size,
has_senior = sum(ifelse(elderly > 0, "Has Senior", "No Senior"))
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size,
has_senior = sum(ifelse(elderly > 0, 1, 0))
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
summary_by_PUMA
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size,
has_senior = sum(ifelse(elderly > 0, 1, 0)),
proportion_has_senior = has_senior/sample_size
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = has_senior)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Iowa Residents on Food Stamps or SNAP",
fill = "Proportion of\nResidents")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_has_senior)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Iowa Residents on Food Stamps or SNAP",
fill = "Proportion of\nResidents")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_has_senior)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households with Senior",
fill = "Proportion of\Households with\nSeniors")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_has_senior)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households with Senior",
fill = "Proportion of\nHouseholds with\nSeniors")
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size,
only_seniors = sum(ifelse(elderly == hhsize, 1, 0)),
proportion_only_senior = only_senior/sample_size,
has_senior = sum(ifelse(elderly > 0, 1, 0)),
proportion_has_senior = has_senior/sample_size
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
total_weights = sum(weight),
total_weights_by_sample = sum(weight *hhsize),
people_on_assistance = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)),
people_on_assistance_weighted = sum(ifelse(fsstmp_prediction == "On Assistance", 1, 0)*weight),
proportion_on_assistance = people_on_assistance/sample_size,
only_senior = sum(ifelse(elderly == hhsize, 1, 0)),
proportion_only_senior = only_senior/sample_size,
has_senior = sum(ifelse(elderly > 0, 1, 0)),
proportion_has_senior = has_senior/sample_size
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_only_senior)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households with Senior",
fill = "Proportion of\nHouseholds with\nSeniors")
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = reduced_train[col1] * reduced_train[col2])
interaction_test = reduced_test %>%
mutate(interaction_term = reduced_test[col1] * reduced_test[col2])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
lr_lasso_fsstmp_cv <- cv.glmnet(proper_train_x, FSSTMP.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
best_lasso_lambda_fsstmp <- lr_lasso_fsstmp_cv$lambda.min #GOOD
lr_lasso_fsstmp <- glmnet(proper_train_x, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lasso_fsstmp_rocCurve <- roc(response = as.factor(FSSTMP.y.test),
predictor = predict(lr_lasso_fsstmp, proper_test_x, type="response")[,1],
levels=c("0", "1"))
interaction_df[inc, "added_interaction"] = str
interaction_df[inc, "AUC"] = lasso_fsstmp_rocCurve$auc
inc = inc + 1
}
}
interaction_train
reduced_train[col1]
cps_data <- as.data.frame(cps_data)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
reduced_train[col1]
reduced_train
###############################
#       Train Test Split      #
###############################
=======
library(randomForest)
cps_data <- read.csv("./data/interim/cps_data.csv")
summary(cps_data)
head(cps_data)
#######################################
############# CLUSTERING ##############
#######################################
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure clsuter-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
# save clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
##########################################
############ Train Test Split ############
#########################################
# splitting training and testing
>>>>>>> Stashed changes
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
<<<<<<< Updated upstream
test.df.preds <- test.df
###########################
#   Food Stamp Analysis   #
###########################
#Things to think about/do
#Fit a random forest
#Fit a cluster?
#What are the weights doing?
#Think about what I could do with NA values
#Make plots/clean up ROC plots
###########################
#   Train Test Split      #
###########################
FSSTMP.x.train <- model.matrix(FSSTMPVALC_bin ~ hhsize + married +
education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSSTMP.x.test <- model.matrix(FSSTMPVALC_bin ~ hhsize + married +
education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = reduced_train[col1] * reduced_train[col2])
interaction_test = reduced_test %>%
mutate(interaction_term = reduced_test[col1] * reduced_test[col2])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
lr_lasso_fsstmp_cv <- cv.glmnet(proper_train_x, FSSTMP.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
best_lasso_lambda_fsstmp <- lr_lasso_fsstmp_cv$lambda.min #GOOD
lr_lasso_fsstmp <- glmnet(proper_train_x, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lasso_fsstmp_rocCurve <- roc(response = as.factor(FSSTMP.y.test),
predictor = predict(lr_lasso_fsstmp, proper_test_x, type="response")[,1],
levels=c("0", "1"))
interaction_df[inc, "added_interaction"] = str
interaction_df[inc, "AUC"] = lasso_fsstmp_rocCurve$auc
inc = inc + 1
}
}
reduced_train
reduced_train[col1]
reduced_train[col1]*reduced_train[col2]
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = reduced_train[col1] * reduced_train[col2])
interaction_test = reduced_test %>%
mutate(interaction_term = reduced_test[col1] * reduced_test[col2])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
lr_lasso_fsstmp_cv <- cv.glmnet(proper_train_x, FSSTMP.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
best_lasso_lambda_fsstmp <- lr_lasso_fsstmp_cv$lambda.min #GOOD
lr_lasso_fsstmp <- glmnet(proper_train_x, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lasso_fsstmp_rocCurve <- roc(response = as.factor(FSSTMP.y.test),
predictor = predict(lr_lasso_fsstmp, proper_test_x, type="response")[,1],
levels=c("0", "1"))
interaction_df[inc, "added_interaction"] = str
interaction_df[inc, "AUC"] = lasso_fsstmp_rocCurve$auc
inc = inc + 1
}
}
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = reduced_train[col1] * reduced_train[col2])
interaction_test = reduced_test %>%
mutate(interaction_term = reduced_test[col1] * reduced_test[col2])
inc = inc + 1
}
}
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = reduced_train[col1] * reduced_train[col2])
interaction_test = reduced_test %>%
mutate(interaction_term = reduced_test[col1] * reduced_test[col2])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
inc = inc + 1
}
}
interaction_train
str(interaction_train)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = as.vector(reduced_train[col1]) * as.vector(reduced_train[col2]))
interaction_test = reduced_test %>%
mutate(interaction_term = reduced_test[col1] * reduced_test[col2])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
inc = inc + 1
}
}
interaction_train = reduced_train %>%
mutate(interaction_term = as.numeric(reduced_train[col1]) * as.numeric(reduced_train[col2]))
interaction_train
str(interaction_train)
reduced_train[col1] * reduced_train[col2]
(reduced_train[col1] * reduced_train[col2])[,1]
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
interaction_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
inc = inc + 1
}
}
str(interaction_train)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:2){
for (j in 1:2){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
interaction_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
inc = inc + 1
}
}
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:2){
for (j in 1:2){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
str = paste(col1, col2, sep="_")
interaction_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
interaction_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
proper_train_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_train)[,-1]
proper_test_x = model.matrix(FSSTMPVALC_bin ~ .
, data=interaction_test)[,-1]
lr_lasso_fsstmp_cv <- cv.glmnet(proper_train_x, FSSTMP.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
best_lasso_lambda_fsstmp <- lr_lasso_fsstmp_cv$lambda.min #GOOD
lr_lasso_fsstmp <- glmnet(proper_train_x, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lasso_fsstmp_rocCurve <- roc(response = as.factor(FSSTMP.y.test),
predictor = predict(lr_lasso_fsstmp, proper_test_x, type="response")[,1],
levels=c("0", "1"))
interaction_df[inc, "added_interaction"] = str
interaction_df[inc, "AUC"] = lasso_fsstmp_rocCurve$auc
inc = inc + 1
}
}
warnings()
=======
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
# RF model
rf_anxietylevel <- randomForest(FSWROUTY ~ county + weight + hhsize + female +
hispanic + black + kids + elderly +
education + married,
data = train.df,
ntree = 100,
mtry = 5)
pi_hat <- predict(rf_anxietylvel, test.df, type = "prob")[, "1"] #Choose positive event column
pi_hat <- predict(rf_anxietylevel, test.df, type = "prob")[, "1"] #Choose positive event column
pi_hat <- predict(rf_anxietylevel, test.df)[, "1"] #Choose positive event column
pi_hat <- predict(rf_anxietylevel, test.df)[, 1] #Choose positive event column
rf_anxietylevel <- randomForest(FSWROUTY ~ county + weight + hhsize + female +
hispanic + black + kids + elderly +
education + married,
data = train.df,
ntree = 100,
mtry = 5,
type = "classification")
rf_anxietylevel <- randomForest(FSWROUTY ~ county + weight + hhsize + female +
hispanic + black + kids + elderly +
education + married,
data = train.df,
ntree = 100,
mtry = 5)
cps_data <- cps_data %>% mutate(
FSWROUTY_char = as.character(FSWROUTY)
)
rf_anxietylevel <- randomForest(FSWROUTY ~ county + weight + hhsize + female +
hispanic + black + kids + elderly +
education + married,
data = train.df,
ntree = 100,
mtry = 5)
pi_hat <- predict(rf_anxietylevel, test.df, type = "prob")[, "1"] #Choose positive event column
pi_hat <- predict(rf_anxietylevel, test.df)[, "1"] #Choose positive event column
pi_hat <- predict(rf_anxietylevel, test.df, type = "prob")
# RF model (regression)
rf_anxietylevel <- randomForest(FSWROUTY ~ county + weight + hhsize + female +
hispanic + black + kids + elderly +
education + married + h_cluster,
data = train.df,
ntree = 100,
mtry = 5)
# RF model
# RF model (regression)
rf_anxietylevel <- randomForest(FSWROUTY ~ county + weight + hhsize + female +
hispanic + black + kids + elderly +
education + married,
data = train.df,
ntree = 100,
mtry = 5)
# Predict on the test set
predicted_FSWROUTY <- predict(rf_anxietylevel, test.df)
# Check the first few predictions
head(predicted_FSWROUTY)
# Feature importance
importance <- varImpPlot(rf_anxietylevel)
# Visualize importance
importance_df <- as.data.frame(importance)
ggplot(importance_df, aes(x = reorder(Var1, Overall), y = Overall)) +
geom_bar(stat = 'identity') +
coord_flip() +
theme_minimal() +
labs(title = "Feature Importance", x = "Features", y = "Importance")
rm(list=ls())
gc()
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
cps_data <- read.csv("./data/interim/cps_data.csv")
summary(cps_data)
head(cps_data)
#######################################
############# CLUSTERING ##############
#######################################
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure clsuter-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
# save clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
##########################################
############ Train Test Split ############
#########################################
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.test <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSWROUTY.y.train <- as.vector(train.df$FSWROUTY)
FSWROUTY.y.test <- as.vector(test.df$FSWROUTY)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
lr_lasso <- cv.glmnet(FSWROUTY.x.train, FSWROUTY.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
plot(lr_lasso)
best_lasso_lambda <- lr_lasso$lambda.min
best_lasso_lambda
lr_lasso_coefs <- coef(lr_lasso, s="lambda.min") %>% as.matrix()
lr_lasso_coefs <- coef(lr_lasso, s="lambda.min") %>% as.matrix()
lr_lasso_coefs
lr_lasso <- glmnet(FSSTMP.x.train, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lr_lasso <- glmnet(FSWROUTY.x.train, FSWROUTY.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda, weights=train.weights)
lr_lasso <- glmnet(FSWROUTY.x.train, FSWROUTY.y.train, family = "multinomial",
alpha=1, lambda = best_lasso_lambda, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_FSWROUTY_preds = predict(lr_lasso, FSWROUTY.x.test, type="response")[,1]
)
test.df.preds <- test.df.preds %>% mutate(
lasso_FSWROUTY_preds = predict(lr_lasso, FSWROUTY.x.test, type="response"))
lasso_FSWROUTY_rocCurve <- roc(response = as.factor(test.df.preds$FSWROUTY),
predictor =test.df.preds$lasso_FSWROUTY_preds,
levels=c("0", "1"))
lasso_FSWROUTY_rocCurve <- roc(response = as.factor(test.df.preds$FSWROUTY),
predictor =test.df.preds$lasso_FSWROUTY_preds)
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
cps_data <- source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
columns(cps_data)
column(cps_data)
colnames(cps_data)
summary(cps_data)
head(cps_data)
cps_data <- source("code/clean_cps.R")
summary(cps_data)
source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
str(data_X)
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
cps_data$county <- as.character(cps_data$county)
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
summary(cps_data)
str(cps_data)
data_X <- cps_data[, c() "weight", "hhsize", "female", "hispanic", "black",
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure clsuter-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.test <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSWROUTY.y.train <- as.vector(train.df$FSWROUTY)
FSWROUTY.y.test <- as.vector(test.df$FSWROUTY)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
tempforest <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 4)
dim(train.df)
tempforest <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 5)
dim(train.df)
mtry <- seq(from = 1, to = 21, by = 3) # to slim down number of tuning options
keeps <- data.frame(m = rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY.x.train,
data = train.df,
ntree= 1000,
mtry = mtry[idx])
# record iteration's m value in idx th row
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$luxury)
}
mtry <- seq(from = 1, to = 21, by = 3) # to slim down number of tuning options
keeps <- data.frame(m = rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY.x.train,
data = train.df,
ntree= 1000,
mtry = mtry[idx])
# record iteration's m value in idx th row
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$luxury)
}
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
str(cps_data)
str(train.df)
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.train
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
rf_fswrouty <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 5,
importance = TRUE)
dim(train.df)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")
pi_hat <- predict(rf_fswrouty, FSWROUTY.x.test, type = "prob")[, "1"]
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 5,
importance = TRUE)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
cps_data$FSWROUTY <- ifelse(0, "No", "Yes")
cps_data$FSWROUTY_bin <- ifelse(0, "No", "Yes")
cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_bin))
summary(cps_data)
cps_data$FSWROUTY_bin <- ifelse(cps_data$FSWROTY == 0, "No", "Yes")
# change y-variable into factor
cps_data$FSWROUTY_bin <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
cps_data$FSWROUTY_bin
summary(cps_data)
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
summary(cps_data)
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
source("code/clean_cps.R")
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
summary(cps_data)
# handle missing values
cps_data$FSWROUTY_binchar <- na.omit(cps_data$FSWROUTY_binchar)
# handle missing values
cps_data$FSWROUTY_bin <- na.omit(cps_data$FSWROUTY_binchar)
# handle missing values
cps_data <- na.omit(cps_data$FSWROUTY_binchar)
summary(cps_data)
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
gc()
rm(list=ls())
source("code/clean_cps.R")
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
summary(cps_data)
cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_bin))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
summary(cps_data)
head(cps_data)
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
# set up
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
test.df.preds <- test.df
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
rf_fswrouty <- randomForest(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "Yes"]
rocCurve <- roc(response = test.df$FSWROUTY_bin,
predictor = pi_hat,
levels = c("No", "Yes"))
plot(rocCurve, print.thres = TRUE, print.ouc = TRUE)
plot(rocCurve, print.thres = TRUE, print.ouc = TRUE)
auc(rocCurve)
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'))
warnings()
beta <- fswrouty_mle %>% coef()
beta
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'), weights = weight)
>>>>>>> Stashed changes
