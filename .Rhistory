<<<<<<< HEAD
head(cps_data)
#is there a point to making this a character and then a factor?
##YES because random forest won't work on numeric, it works on factors
##because then it sees it as a classification, not regression, problem.
#there's a bunch of nulls in here and it's generally binary, currently num type
#just going to drop the na's and take note of how many obs I
#had and how many are left
##TASKS
#try regressions both with and without some meaningful interaction
#terms to look at the ways your X's could affect eachother and the outcome
summary(cps_data$FSFOODS) #FSFOODS has 1755 NA's
cps_data_f <- cps_data[!is.na(cps_data$FSFOODS),]
summary(cps_data_f$FSFOODS) #new subset without NAs has 6665 obs, compared to 8420 originally
summary(cps_data_f)
#Split the data into train/test df forms to use in lasso/ridge later
RNGkind(sample.kind = "default")
set.seed(26473)
train.idx <- sample(x = 1:nrow(cps_data_f), size = .7*nrow(cps_data_f))
train.df <- cps_data_f[train.idx,]
test.df <- cps_data_f[-train.idx,]
x_vars = c("hhsize", "female", "hispanic", "black", "faminc_cleaned",
"kids", "elderly", "education", "married", "donut")
y_var = c("FSFOODS")
#----MLE Logistic Regression----
lr_mle_fsfoods <- glm(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=train.df,
=======
<<<<<<< Updated upstream
source("./code/clean_acs.R")
include_squared_interaction = FALSE
cps_data <- as.data.frame(cps_data)
cps_data <- cps_data %>% mutate(
FSSTMPVALC_bin_fact = as.factor(FSSTMPVALC_bin_char),
donut = as.factor(donut)
)
#(specificity, sensitivity)
###############################
#       Train Test Split      #
###############################
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
test.df.preds <- test.df
x_vars = c("hhsize", "female", "hispanic", "black", "faminc_cleaned",
"kids", "elderly", "education", "married", "donut")
y_var = c("FSSTMPVALC_bin")
###########################
#   Food Stamp Analysis   #
###########################
#Create more visualizations
#Choose a model to use
#Combine FSSTMP and WROUTY to see if there are big differences
#Add weighted means
###########################################
##  Adding all interaction/squared terms ##
###########################################
reduced_train = train.df %>%
select(c(x_vars, y_var))
reduced_test = test.df %>%
select(c(x_vars, y_var))
#With or without interactions/squared terms
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
}
###########################
#   Train Test Split      #
###########################
FSSTMP.x.train <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_train)[,-1]
FSSTMP.x.test <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_test)[,-1]
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
###########################
# MLE Logistic Regression #
###########################
lr_mle_fsstmp <- glm(FSSTMPVALC_bin ~ .,
data=reduced_train,
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
family=binomial(link="logit"),
weights=train.weights
)
<<<<<<< HEAD
# Get warnings - algorithm did not converge, complete separation occurred
summary(lr_mle_fsfoods) #grossly high standard error on all vars, confirms complete separation
#look at the coefficients from the MLE logistic regression
lr_mle_fsfoods_beta <- lr_mle_fsfoods %>% coef()
#---Firth's Penalized Likelihood----
lr_fmle_fsfoods <- logistf(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=train.df,
weights=train.weights)
#clean FSFOODS analysis file
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(dplyr)
library(rpart)
library(rpart.plot)
library(ggplot2)
source("./code/clean_cps.R")
#source("./code/clean_acs.R") shouldn't I only have the cps source?
#Cleaning and Exploring
summary(cps_data)
str(cps_data)
head(cps_data)
#is there a point to making this a character and then a factor?
##YES because random forest won't work on numeric, it works on factors
##because then it sees it as a classification, not regression, problem.
#there's a bunch of nulls in here and it's generally binary, currently num type
#just going to drop the na's and take note of how many obs I
#had and how many are left
##TASKS
#try regressions both with and without some meaningful interaction
#terms to look at the ways your X's could affect eachother and the outcome
summary(cps_data$FSFOODS) #FSFOODS has 1755 NA's
cps_data_f <- cps_data[!is.na(cps_data$FSFOODS),]
summary(cps_data_f$FSFOODS) #new subset without NAs has 6665 obs, compared to 8420 originally
summary(cps_data_f)
#Split the data into train/test df forms to use in lasso/ridge later
RNGkind(sample.kind = "default")
set.seed(26473)
train.idx <- sample(x = 1:nrow(cps_data_f), size = .7*nrow(cps_data_f))
train.df <- cps_data_f[train.idx,]
test.df <- cps_data_f[-train.idx,]
x_vars = c("hhsize", "female", "hispanic", "black", "faminc_cleaned",
"kids", "elderly", "education", "married", "donut")
y_var = c("FSFOODS")
## Make all necessary matrices and vectors
fsfoods.x.train <- model.matrix(FSFOODS~hhsize + married + education + elderly +
kids + black + hispanic + female+ faminc_cleaned,
data = train.df)[,-1]
fsfoods.x.test <- model.matrix(FSFOODS~hhsize + married + education + elderly +
kids + black + hispanic + female+ faminc_cleaned,
data = test.df)[,-1]
fsfoods.y.train <- train.df$FSFOODS %>% as.vector()
fsfoods.y.test <- test.df$FSFOODS %>% as.vector()
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight) #not strictly necessary, for ease of reference
#----MLE Logistic Regression----
lr_mle_fsfoods <- glm(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=train.df,
family=binomial(link="logit"),
weights=weight
)
# Get warnings - algorithm did not converge, complete separation occurred
summary(lr_mle_fsfoods) #grossly high standard error on all vars, confirms complete separation
#look at the coefficients from the MLE logistic regression
lr_mle_fsfoods_beta <- lr_mle_fsfoods %>% coef()
#---Firth's Penalized Likelihood----
lr_fmle_fsfoods <- logistf(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=train.df,
weights=train.weights)
summary(lr_fmle_fsfoods)
#look at the coefficients from the MLE logistic regression
lr_fmle_fsfoods_beta <- lr_fmle_fsfoods %>% coef()
#----Lasso and Ridge with Basic X vars----
#Use cross validation to get tuning info for final regression
fsfoods_lasso_cv <- cv.glmnet(fsfoods.x.train, #MATRIX without our Y COLUMN
fsfoods.y.train, #VECTOR - our Y COLUMN
family = binomial(link = "logit"),
alpha = 1,
weights = train.weights #1 for lasso, 0 for ridge
)
fsfoods_ridge_cv <- cv.glmnet(fsfoods.x.train, #MATRIX without our Y COLUMN
fsfoods.y.train, #VECTOR - our Y COLUMN
family = binomial(link = "logit"),
alpha = 0,
weights = train.weights#1 for lasso, 0 for ridge
)
#Find and extract minimizing lambda values
plot(fsfoods_lasso_cv)
plot(fsfoods_ridge_cv)
best_lasso_lambda <- fsfoods_lasso_cv$lambda.min
best_ridge_lambda <- fsfoods_ridge_cv$lambda.min
#fit final lasso + ridge models
fsfoods_lasso_f1 <- glmnet(fsfoods.x.train, fsfoods.y.train,
family = binomial(link = "logit"), alpha = 1,
lambda = best_lasso_lambda) #this lambda is what actually tunes the model
fsfoods_ridge_f1 <- glmnet(fsfoods.x.train, fsfoods.y.train,
family = binomial(link = "logit"), alpha = 0,
lambda = best_ridge_lambda) #this lambda is what actually tunes the model
#----Random Forest----
#Create big tree, then prune
set.seed(123432456)
ctree <- rpart(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female+ faminc_cleaned,
data = train.df, #training data, NOT original data
method = "class",
control = rpart.control(cp = 0.0001, minsplit = 1))
printcp(ctree)
optimalcp <- ctree$cptable[which.min(ctree$cptable[,"xerror"]),"CP"]
#gives you the optimal complexity parameter (cp of tree with smallest xerror)
#prune to create tuned tree
ctree2 <- prune(ctree, cp = optimalcp)
rpart.plot(ctree2)
#----Compare Models' Performances----
#First, on the testing data split
test.df.cpspreds <- test.df %>%
mutate(
mle_pred = predict(lr_mle_fsfoods, test.df, type = "response"),
fmle_pred = predict(lr_fmle_fsfoods, test.df, type = "response"),
lasso_pred = predict(fsfoods_lasso_f1, fsfoods.x.test, type = "response")[,1],
ridge_pred = predict(fsfoods_ridge_f1, fsfoods.x.test, type = "response")[,1]
forest_pred = predict(ctree2, test.df, type = "prob")
test.df.cpspreds <- test.df %>%
mutate(
mle_pred = predict(lr_mle_fsfoods, test.df, type = "response"),
fmle_pred = predict(lr_fmle_fsfoods, test.df, type = "response"),
lasso_pred = predict(fsfoods_lasso_f1, fsfoods.x.test, type = "response")[,1],
ridge_pred = predict(fsfoods_ridge_f1, fsfoods.x.test, type = "response")[,1],
forest_pred = predict(ctree2, test.df, type = "prob")
)
#Fit ROC Curves on CPS
mle_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$mle_pred,
levels = c("0", "1"))
fmle_rocCurve<- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$fmle_pred,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$lasso_pred,
levels = c("0", "1"))
ridge_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$ridge_pred,
levels = c("0", "1"))
forest_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$forest_pred,
levels = c("0", "1"))
forest_rocCurve <- roc(response = test.df.cpspreds$FSFOODS,
predictor = test.df.cpspreds$forest_pred,
levels = c("0", "1"))
rf_train <- train.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS = train.df$FSFOODS
)
rf_test <- test.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS = test.df$FSFOODS
)
rf_init_fsfoods <- randomForest(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
mtry=floor(sqrt(length(x_vars))),
ntree=1000,
importance=TRUE)
=======
summary(lr_mle_fsstmp) #Complete separation because of EXTREMELY high standard errors
lr_mle_fsstmp_beta <- lr_mle_fsstmp %>% coef()
test.df.preds <- test.df.preds %>%
mutate(
lr_mle_fsstmp_preds = predict(lr_mle_fsstmp, reduced_test, type="response")
)
lr_mle_fsstmp_rocCurve <- roc(
response=as.factor(test.df.preds$FSSTMPVALC_bin),
predictor= test.df.preds$lr_mle_fsstmp_preds,
levels=c("0","1"))
plot(lr_mle_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #.514 AUC (.978, .049) THIS IS REALLY BAD
lr_mle_fsstmp_pi_star <- coords(lr_mle_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
################################
# Firth's Penalized Likelihood #
################################
lr_firths_fsstmp <- logistf(FSSTMPVALC_bin ~ .,
data=reduced_train,
weights=train.weights)
summary(lr_firths_fsstmp)
lr_firths_fsstmp_beta <- lr_firths_fsstmp %>% coef()
test.df.preds <- test.df.preds %>% mutate(
lr_firths_fsstmp_preds = predict(lr_firths_fsstmp, reduced_test, type="response")
)
lr_firths_fsstmp_rocCurve <- roc(
response=as.factor(test.df.preds$FSSTMPVALC_bin),
predictor= test.df.preds$lr_firths_fsstmp_preds,
levels=c("0","1"))
plot(lr_firths_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #.798 AUC (.690, .803)
lr_firths_fsstmp_pi_star <- coords(lr_firths_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
################
#     Lasso    #
################
lr_lasso_fsstmp_cv <- cv.glmnet(FSSTMP.x.train, FSSTMP.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
plot(lr_lasso_fsstmp_cv)
best_lasso_lambda_fsstmp <- lr_lasso_fsstmp_cv$lambda.min
lr_lasso_coefs_fsstmp <- coef(lr_lasso_fsstmp_cv, s="lambda.min") %>% as.matrix()
lr_lasso_coefs_fsstmp #hispanic and hhsize both went to 0
lr_lasso_fsstmp <- glmnet(FSSTMP.x.train, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_fsstmp_preds = predict(lr_lasso_fsstmp, FSSTMP.x.test, type="response")[,1]
)
lasso_fsstmp_rocCurve <- roc(response = as.factor(test.df.preds$FSSTMPVALC_bin),
predictor =test.df.preds$lasso_fsstmp_preds,
levels=c("0", "1"))
plot(lasso_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #Better at AUC = .798, (.681, .810)
lasso_fsstmp_pi_star <- coords(lasso_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
#############
#   Ridge   #
#############
lr_ridge_fsstmp_cv <- cv.glmnet(FSSTMP.x.train, FSSTMP.y.train,
family=binomial(link="logit"), alpha=0, weights=train.weights)
plot(lr_ridge_fsstmp_cv)
best_ridge_lambda_fsstmp <- lr_lasso_fsstmp_cv$lambda.min
lr_ridge_coefs_fsstmp <- coef(lr_ridge_fsstmp_cv, s="lambda.min") %>% as.matrix()
lr_ridge_coefs_fsstmp
lr_ridge_fsstmp <- glmnet(FSSTMP.x.train, FSSTMP.y.train, family=binomial(link="logit"),
alpha=0, lambda = best_ridge_lambda_fsstmp, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
ridge_fsstmp_preds = predict(lr_ridge_fsstmp, FSSTMP.x.test, type="response")[,1]
)
ridge_fsstmp_rocCurve <- roc(response = as.factor(test.df.preds$FSSTMPVALC_bin),
predictor =test.df.preds$ridge_fsstmp_preds,
levels=c("0", "1"))
plot(ridge_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #.800 (.684,.810)
ridge_fsstmp_pi_star <- coords(ridge_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
#########################
#     Random Forest     #
#########################
rf_train <- reduced_train %>% select(-c("FSSTMPVALC_bin")) %>%
mutate(
FSSTMPVALC_bin_fact = train.df$FSSTMPVALC_bin_fact
)
rf_test <- reduced_test %>% select(-c("FSSTMPVALC_bin")) %>%
mutate(
FSSTMPVALC_bin_fact = test.df$FSSTMPVALC_bin_fact
)
rf_init_fsstmp <- randomForest(FSSTMPVALC_bin_fact ~ .,
data=rf_train,
mtry=floor(sqrt(length(x_vars))),
ntree=1000,
importance=TRUE)
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSSTMPVALC_bin_fact ~.,
data=rf_train,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != rf_train$FSSTMPVALC_bin_fact) #Estimates out of sample error
}
best_m <- keeps[order(keeps$OOB_err_rate),"m"][1]
final_forest <- randomForest(FSSTMPVALC_bin_fact ~ .,
data=rf_train,
mtry=best_m,
ntree=1000,
importance=TRUE)
pi_hat <- predict(final_forest, rf_test, type="prob")[,"Yes"]
rf_rocCurve <- roc(response=rf_test$FSSTMPVALC_bin_fact,
predictor=pi_hat,
levels=c("No", "Yes"))
plot(rf_rocCurve, print.thres=TRUE, print.auc=TRUE)
rf_fsstmp_pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test.df.preds <- test.df.preds %>% mutate(
rf_fsstmp_preds = as.factor(ifelse(pi_hat > rf_fsstmp_pi_star, "Yes", "No"))
)
varImpPlot(final_forest, type=1)
rf_vi <- as.data.frame(varImpPlot(final_forest, type=1))
rf_vi$Variable <- rownames(rf_vi)
rf_vi <- rf_vi %>% arrange(desc(MeanDecreaseAccuracy))
#####################
# Tree              #
#####################
ctree <- rpart(FSSTMPVALC_bin_fact ~ .,
data=rf_train,
method="class",
control=rpart.control(cp=.0001, minsplit=1))
optimalcp <- ctree$cptable[which.min(ctree$cptable[,"xerror"]), "CP"]
ctree_optimal <- prune(ctree, cp=optimalcp)
rpart.plot(ctree_optimal)
pi_hat <- predict(ctree_optimal, rf_test, type="prob")[,"Yes"]
ctree_rocCurve <- roc(response=rf_test$FSSTMPVALC_bin_fact,
predictor=pi_hat,
levels=c("No", "Yes"))
plot(ctree_rocCurve, print.thres=TRUE, print.auc=TRUE)
ctree_fsstmp_pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test.df.preds <- test.df.preds %>% mutate(
ctree_fsstmp_preds = as.factor(ifelse(pi_hat > ctree_fsstmp_pi_star, "Yes", "No"))
)
###################################
# PREDICTING FSWROUTY AND FSSTMP  #
###################################
#######################
## COMPARE ALL AUCS ##
######################
mle_lr_auc <- data.frame(
Model = "MLE LR",
Specificity = lr_mle_fsstmp_rocCurve$specificities,
Sensitivity = lr_mle_fsstmp_rocCurve$sensitivities,
AUC = lr_mle_fsstmp_rocCurve$auc %>% as.numeric
)
firths_lr_auc <- data.frame(
Model = "Firths LR",
Specificity = lr_firths_fsstmp_rocCurve$specificities,
Sensitivity = lr_firths_fsstmp_rocCurve$sensitivities,
AUC = lr_firths_fsstmp_rocCurve$auc %>% as.numeric
)
lasso_lr_auc <- data.frame(
Model = "Lasso LR",
Specificity = lasso_fsstmp_rocCurve$specificities,
Sensitivity = lasso_fsstmp_rocCurve$sensitivities,
AUC = lasso_fsstmp_rocCurve$auc %>% as.numeric
)
ridge_lr_auc <- data.frame(
Model = "Ridge LR",
Specificity = ridge_fsstmp_rocCurve$specificities,
Sensitivity = ridge_fsstmp_rocCurve$sensitivities,
AUC = ridge_fsstmp_rocCurve$auc %>% as.numeric
)
ctree_auc <- data.frame(
Model = "Categorical Tree",
Specificity = ctree_rocCurve$specificities,
Sensitivity = ctree_rocCurve$sensitivities,
AUC = ctree_rocCurve$auc %>% as.numeric
)
rf_auc <- data.frame(
Model = "Random Forest",
Specificity = rf_rocCurve$specificities,
Sensitivity = rf_rocCurve$sensitivities,
AUC = rf_rocCurve$auc %>% as.numeric
)
roc_data <- rbind(mle_lr_auc, firths_lr_auc, lasso_lr_auc, ridge_lr_auc, ctree_auc, rf_auc)
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
################################
#   MAKING PREDICTIONS ON ACS  #
################################
#Add all squared/interaction terms to ACS data
acs_reduced_test = acs_data %>%
select(x_vars) %>%
mutate(
donut = as.factor(donut)
)
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 = colnames(acs_reduced_test)[i][1]
col2 = colnames(acs_reduced_test)[j][1]
col_str = paste(col1, col2, sep="_")
acs_reduced_test = acs_reduced_test %>%
mutate(interaction_term = (acs_reduced_test[col1] * acs_reduced_test[col2])[,1])
names(acs_reduced_test)[names(acs_reduced_test) == "interaction_term"] = col_str
}
}
}
acs_test_data <- model.matrix(~., data=acs_reduced_test)[,-1]
fsstmp_predictions <- predict(lr_lasso_fsstmp, acs_test_data, type="response")[,1]
fsstmp_predictions
acs_predicted <- acs_data %>% mutate(
fsstmp_prediction = ifelse(fsstmp_predictions > lasso_fsstmp_pi_star, "On Assistance", "Not On Assistance")
)
acs_prediction
acs_predicted
acs_predicted <- acs_data %>% mutate(
fsstmp_prediction = ifelse(fsstmp_predictions > lasso_fsstmp_pi_star, "On Assistance", "Not On Assistance"),
fsstmp_prediction_bin = ifelse(fsstmp_predictions > lasso_fsstmp_pi_star, 1,0)
)
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
proportion_on_assistance = weighted.mean(fsstmp_prediction, weight),
only_senior = sum(ifelse(elderly == hhsize, 1, 0)),
proportion_only_senior = weighted.mean(only_senior, weight),
has_senior = sum(ifelse(elderly > 0, 1, 0)),
proportion_has_senior = weighted.mean(has_senior, weight)
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
proportion_on_assistance = weighted.mean(fsstmp_prediction_bin, weight),
only_senior = sum(ifelse(elderly == hhsize, 1, 0)),
has_senior = sum(ifelse(elderly > 0, 1, 0))
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
mean(fsstmp_prediction_bin)
mean(acs_predicted$fsstmp_prediction_bin)
table(acs_predicted$fsstmp_prediction)
head(acs_reducted_test)
acs_reduced_test
plot(lasso_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #Better at AUC = .798, (.681, .810)
acs_test_data <- model.matrix(~., data=acs_reduced_test)[,-1]
fsstmp_predictions <- predict(lr_ridge_fsstmp, acs_test_data, type="response")[,1]
acs_predicted <- acs_data %>% mutate(
fsstmp_prediction = ifelse(fsstmp_predictions > ridge_fsstmp_pi_star, "On Assistance", "Not On Assistance"),
fsstmp_prediction_bin = ifelse(fsstmp_predictions > ridge_fsstmp_pi_star, 1,0)
)
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
proportion_on_assistance = weighted.mean(fsstmp_prediction_bin, weight),
only_senior = sum(ifelse(elderly == hhsize, 1, 0)),
has_senior = sum(ifelse(elderly > 0, 1, 0))
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
table(test.df.preds$lasso_fsstmp_preds)
mean(test.df.preds$lasso_fsstmp_preds)
mean(ifelse(test.df.preds$lasso_fsstmp_preds > lasso_fsstmp_pi_star), 1, 0)
mean(ifelse(test.df.preds$lasso_fsstmp_preds > lasso_fsstmp_pi_star, 1, 0))
mean(train.df$FSSTMPVALC_bin)
source("C:/Users/mattc/Desktop/FA24/STAT172Final/code/fsstamp_analysis.R")
acs_reduced_test = acs_data %>%
select(x_vars) %>%
#mutate(
#  donut = as.factor(donut)
#)
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 = colnames(acs_reduced_test)[i][1]
col2 = colnames(acs_reduced_test)[j][1]
col_str = paste(col1, col2, sep="_")
acs_reduced_test = acs_reduced_test %>%
mutate(interaction_term = (acs_reduced_test[col1] * acs_reduced_test[col2])[,1])
names(acs_reduced_test)[names(acs_reduced_test) == "interaction_term"] = col_str
}
}
}
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 = colnames(acs_reduced_test)[i][1]
col2 = colnames(acs_reduced_test)[j][1]
col_str = paste(col1, col2, sep="_")
acs_reduced_test = acs_reduced_test %>%
mutate(interaction_term = (acs_reduced_test[col1] * acs_reduced_test[col2])[,1])
names(acs_reduced_test)[names(acs_reduced_test) == "interaction_term"] = col_str
}
}
}
acs_reduced_test = acs_data %>%
select(x_vars)
acs_test_data <- model.matrix(~., data=acs_reduced_test)[,-1]
fsstmp_predictions <- predict(lr_ridge_fsstmp, acs_test_data, type="response")[,1]
acs_predicted <- acs_data %>% mutate(
fsstmp_prediction = ifelse(fsstmp_predictions > ridge_fsstmp_pi_star, "On Assistance", "Not On Assistance"),
fsstmp_prediction_bin = ifelse(fsstmp_predictions > ridge_fsstmp_pi_star, 1,0)
)
#How does this adjust with the weights
summary_by_PUMA <- acs_predicted %>% group_by(PUMA = as.factor(PUMA)) %>%
summarise(
sample_size = sum(hhsize),
proportion_on_assistance = weighted.mean(fsstmp_prediction_bin, weight),
only_senior = sum(ifelse(elderly == hhsize, 1, 0)),
has_senior = sum(ifelse(elderly > 0, 1, 0))
) %>% as.data.frame() %>% arrange(desc(proportion_on_assistance))
#https://www.geoplatform.gov/metadata/258db7ce-2581-4488-bb5e-e387b6119c7a
sf_data <- st_read("./data/tl_2023_19_puma20/tl_2023_19_puma20.shp")
colnames(sf_data)[colnames(sf_data) == "GEOID20"] = "PUMA"
map_data <- sf_data %>%
left_join(summary_by_PUMA, by = "PUMA")
#Predictions are way too high
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_has_senior)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households with Senior",
fill = "Proportion of\nHouseholds with\nSeniors")
#Predictions are way too high
ggplot(data = map_data) +
geom_sf(aes(fill = proportion_on_assistance)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
senior_data <- read.csv("./data/iowa_seniors_by_puma.csv")
senior_data
senior_data <- read.csv("./data/iowa_seniors_by_puma.csv")
colnames(senior_data)[colnames(senior_data) == "GEOID"] = "PUMA"
senior_data <- sf_data %>% left_join(senior_data, by="PUMA")
senior_data <- read.csv("./data/iowa_seniors_by_puma.csv")
senior_data <- senior_data %>% mutate("PUMA" = as.character("GEOID"))
senior_data <- sf_data %>% left_join(senior_data, by="PUMA")
ggplot(data = senior_data) +
geom_sf(aes(fill = senior_population)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
str(senior_data)
senior_data <- read.csv("./data/iowa_seniors_by_puma.csv")
senior_data <- senior_data %>% mutate("PUMA" = as.character("GEOID"))
senior_data
senior_data <- senior_data %>% mutate("PUMA" = as.character(GEOID))
senior_data
senior_data <- sf_data %>% left_join(senior_data, by="PUMA")
ggplot(data = senior_data) +
geom_sf(aes(fill = senior_population)) +
scale_fill_viridis_c(option = "plasma") +  # Adjust color palette as needed
theme_minimal() +
labs(title = "Proportion of Households on SNAP/Food Stamps",
fill = "Proportion on\nFood Stamps/SNAP")
senior_data <- sf_data %>% outer_join(senior_data, by="PUMA")
senior_data <- sf_data %>% join(senior_data, by="PUMA")
senior_data <- sf_data %>% left_join(senior_data, by="PUMA")
senior_data <- senior_data %>% mutate("PUMA" = as.character(GEOID))
senior_data <- sf_data %>% left_join(senior_data, by="PUMA")
senior_data <- read.csv("./data/iowa_seniors_by_puma.csv")
senior_data <- senior_data %>% mutate("PUMA" = as.character(GEOID))
senior_data <- sf_data %>% left_join(senior_data, by="PUMA")
senior_data
senior_data[is.na(senior_data$senior_population),]
install.packages("tidycensus")
library(tidycensus)
census_api_key("a967eecac85aa2ffa23c44da1c1a25298930cf6e", install = TRUE)
readRenviron("~/.Renviron")  # Refresh environment to use the key
iowa_pums <- get_pums(
variables = c("AGEP", "PUMA", "PWGTP"), # Age, PUMA code, and person weight
state = "IA",                          # Iowa state
year = 2022,                           # Adjust year as needed
survey = "acs5"                        # 5-year ACS
)
iowa_pums <- get_pums(
variables = c("AGEP", "PUMA20", "PWGTP"), # Age, PUMA code, and person weight
state = "IA",                          # Iowa state
year = 2022,                           # Adjust year as needed
survey = "acs5"                        # 5-year ACS
)
census_api_key("a967eecac85aa2ffa23c44da1c1a25298930cf6e", install = TRUE)
iowa_pums <- get_pums(
variables = c("AGEP", "PUMA20", "PWGTP"), # Age, PUMA code, and person weight
state = "IA",                          # Iowa state
year = 2022,                           # Adjust year as needed
survey = "acs5"                        # 5-year ACS
)
iowa_pums <- get_pums(
variables = c("AGEP", "PUMA20", "PWGTP"), # Age, PUMA code, and person weight
state = "IA",                          # Iowa state
year = 2022,                           # Adjust year as needed
survey = "acs5"                        # 5-year ACS
)
rm(list=ls())
=======
head(cps_data)
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin","FSWROUTY_binchar", "weight", "hhsize",
"female", "hispanic", "black", "kids", "elderly",
"education", "married", "faminc_cleaned")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
>>>>>>> Stashed changes
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(rpart)
library(rpart.plot)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
<<<<<<< Updated upstream
library(tidycensus)
census_api_key("a967eecac85aa2ffa23c44da1c1a25298930cf6e", install = TRUE)
iowa_pums <- get_pums(
variables = c("AGEP", "PUMA20", "PWGTP"), # Age, PUMA code, and person weight
state = "IA",                          # Iowa state
year = 2022,                           # Adjust year as needed
survey = "acs5"                        # 5-year ACS
=======
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
source("code/clean_cps.R")
############ Clean Y-Variable ############
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
# cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = ifelse(FSWROUTY_binchar == "No", 0, 1))
summary(cps_data)
head(cps_data)
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin", "weight", "hhsize",
"female", "hispanic", "black", "kids", "elderly",
"education", "married", "faminc_cleaned")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
######## Adding Interactions/Squared Terms #########
x_vars = c("hhsize", "female", "hispanic", "black", "kids", "elderly",
"education", "married", "faminc_cleaned")
y_var = c("FSWROUTY_bin")
include_squared_interaction = FALSE
#With or without interactions/squared terms
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 <- colnames(train.df)[i][1]
col2 <- colnames(train.df)[j][1]
col_str <- paste(col1, col2, sep="_")
reduced_train = train.df %>%
mutate(interaction_term = (train.df[col1] * train.df[col2])[,1])
reduced_test = test.df %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
}
rf_fswrouty_temp <- randomForest(as.character(FSWROUTY_binchar) ~ .,
data = train.df,
ntree = 1000,
mtry = floor(sqrt(length(x_vars))),
importance = TRUE)
rf_fswrouty_temp <- randomForest(as.character(FSWROUTY_bin) ~ .,
data = train.df,
ntree = 1000,
mtry = floor(sqrt(length(x_vars))),
importance = TRUE)
rf_fswrouty_temp <- randomForest(as.factor(FSWROUTY_bin) ~ .,
data = train.df,
ntree = 1000,
mtry = floor(sqrt(length(x_vars))),
importance = TRUE)
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY_binchar ~.,
data= train.df,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$FSWROUTY_binchar) #Estimates out of sample error
}
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(as.factor(FSWROUTY_bin) ~.,
data= train.df,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$as.factor(FSWROUTY_bin)) #Estimates out of sample error
}
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(as.factor(FSWROUTY_bin) ~.,
data= train.df,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$FSWROUTY_bin) #Estimates out of sample error
}
best_m <- keeps[order(keeps$OOB_err_rate),"m"][1]
best_m
# final RF model
rf_fswrouty <- randomForest(FSWROUTY_binchar ~ .,
data = train.df,
ntree = 1000,
mtry = best_m,
importance = TRUE)
rf_fswrouty <- randomForest(as.factor(FSWROUTY_bin) ~ .,
data = train.df,
ntree = 1000,
mtry = best_m,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "Yes"] #Choose positive event column
library(ggplot2)
ggplot(keeps, aes(x = m, y = OOB_err_rate)) +
geom_line() +
geom_point() +
labs(title = "OOB Error Rate vs mtry",
x = "mtry",
y = "OOB Error Rate") +
theme_minimal()
levels(train.df$FSWROUTY_bin)
levels(train.df$FSWROUTY_bin)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
rf_rocCurve <- roc(response = test.df$FSWROUTY_binchar,
predictor = pi_hat,
levels = c("No", "Yes"))
rf_rocCurve <- roc(response = test.df$FSWROUTY_binchar,
predictor = pi_hat,
levels = c("0", "1"))
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
rf_rocCurve <- roc(response = test.df$FSWROUTY_binchar,
predictor = pi_hat,
levels = levels(test.df$FSWROUTY_bin))
rf_rocCurve <- roc(response = test.df$FSWROUTY_bin,
predictor = pi_hat,
levels = c("0", "1"))
plot(rf_rocCurve, print.thres = TRUE, print.ouc = TRUE)
auc(rf_rocCurve)
pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test_preds <- test.df %>% mutate(
rf_preds = as.factor(ifelse(pi_hat > pi_star, "Yes", "No"))
)
varImpPlot(rf_fswrouty, type=1)
rf_vi <- as.data.frame(varImpPlot(rf_fswrouty, type=1))
rf_vi$Variable <- rownames(rf_vi)
rf_vi <- rf_vi %>% arrange(desc(MeanDecreaseAccuracy))
ctree <- rpart(FSWROUTY_bin ~ .,
data= train.df,
method="class",
control=rpart.control(cp=.0001, minsplit=1))
optimalcp <- ctree$cptable[which.min(ctree$cptable[,"xerror"]), "CP"]
ctree_optimal <- prune(ctree, cp=optimalcp)
rpart.plot(ctree_optimal)
pi_hat <- predict(ctree_optimal, test.df, type="prob")[,"Yes"]
pi_hat <- predict(ctree_optimal, test.df, type="prob")[,"1"]
ctree_rocCurve <- roc(response=test.df$FSWROUTY_bin,
predictor=pi_hat,
levels=c("0", "1"))
plot(ctree_rocCurve, print.thres=TRUE, print.auc=TRUE)
ctree_pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test_preds <- test_preds %>% mutate(
ctree_preds = as.factor(ifelse(pi_hat > ctree_pi_star, "Yes", "No"))
)
rf_vi$Variable <- rownames(rf_vi)
rownames(rf_vi)
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
source("code/clean_cps.R")
############ Clean Y-Variable ############
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
# cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = ifelse(FSWROUTY_binchar == "No", 0, 1))
summary(cps_data)
head(cps_data)
############# CLUSTERING ##############
# set up
data_X <- cps_data[, c( "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "faminc_cleaned")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin", "weight", "hhsize",
"female", "hispanic", "black", "kids", "elderly",
"education", "married", "faminc_cleaned")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
######## Adding Interactions/Squared Terms #########
x_vars = c("hhsize", "female", "hispanic", "black", "kids", "elderly",
"education", "married", "faminc_cleaned")
y_var = c("FSWROUTY_bin")
include_squared_interaction = FALSE
#With or without interactions/squared terms
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 <- colnames(train.df)[i][1]
col2 <- colnames(train.df)[j][1]
col_str <- paste(col1, col2, sep="_")
reduced_train = train.df %>%
mutate(interaction_term = (train.df[col1] * train.df[col2])[,1])
reduced_test = test.df %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
}
################ MLE #########################
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female + faminc_cleaned,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
#Since all variables are seen as significant because of complete separation
test_preds <- test.df %>%
mutate(
mle_fswrouty_prob = predict(fswrouty_mle, test.df , type="response")
)
mle_fswrouty_rocCurve <- roc(
response=as.factor(test_preds$FSWROUTY_bin),
predictor= test_preds$mle_fswrouty_prob,
levels=c("0","1"))
plot(mle_fswrouty_rocCurve, print.thres=TRUE, print.auc=TRUE)
################## Firth's Penalized Likelihood ###################
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=train.df,
weights=weight)
summary(firths_fswrouty)
firths_fswrouty_beta <- firths_fswrouty %>% coef()
test_preds <- test.df %>%
mutate(
firths_fswrouty_prob = predict(firths_fswrouty, test.df , type="response")
>>>>>>> Stashed changes
)
firths_fswrouty_rocCurve <- roc(
response=as.factor(test_preds$FSWROUTY_bin),
predictor= test_preds$firths_fswrouty_prob,
levels=c("0","1"))
plot(firths_fswrouty_rocCurve, print.thres=TRUE, print.auc=TRUE)
firth_fswrouty_pi_hat <- coords(firths_fswrouty_rocCurve, "best", ref="threshold")$threshold[1]
############# RANDOM FOREST ###################
rf_fswrouty_temp <- randomForest(as.factor(FSWROUTY_bin) ~ .,
data = train.df,
ntree = 1000,
mtry = floor(sqrt(length(x_vars))),
importance = TRUE)
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
<<<<<<< HEAD
tempforest <- randomForest(FSSTMPVALC_bin_fact ~.,
data=rf_train,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != rf_train$FSSTMPVALC_bin_fact) #Estimates out of sample error
}
rf_train <- train.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS = train.df$FSFOODS
)
rf_test <- test.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS = test.df$FSFOODS
)
rf_init_fsfoods <- randomForest(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
mtry=floor(sqrt(length(x_vars))),
ntree=1000,
importance=TRUE)
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSFOODS ~hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != rf_train$FSFOODS) #Estimates out of sample error
}
best_m <- keeps[order(keeps$OOB_err_rate),"m"][1]
final_forest <- randomForest(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
mtry=best_m,
ntree=1000,
importance=TRUE)
best_m
summary(best_m)
#----Compare Models' Performances----
#First, on the testing data split
test.df.cpspreds <- test.df %>%
mutate(
mle_pred = predict(lr_mle_fsfoods, test.df, type = "response"),
fmle_pred = predict(lr_fmle_fsfoods, test.df, type = "response"),
lasso_pred = predict(fsfoods_lasso_f1, fsfoods.x.test, type = "response")[,1],
ridge_pred = predict(fsfoods_ridge_f1, fsfoods.x.test, type = "response")[,1],
forest_pred = predict(final_forest, rf_test, type = "prob")[,1]
#pi_hat <- predict(final_forest, rf_test, type="prob")[,"Yes"]
)
#----Compare Models' Performances----
#First, on the testing data split
test.df.cpspreds <- test.df %>%
mutate(
mle_pred = predict(lr_mle_fsfoods, test.df, type = "response"),
fmle_pred = predict(lr_fmle_fsfoods, test.df, type = "response"),
lasso_pred = predict(fsfoods_lasso_f1, fsfoods.x.test, type = "response")[,1],
ridge_pred = predict(fsfoods_ridge_f1, fsfoods.x.test, type = "response")[,1],
forest_pred = predict(final_forest, rf_test, type = "response")[,1]
#pi_hat <- predict(final_forest, rf_test, type="prob")[,"Yes"]
)
pi_hat <- predict(final_forest, rf_test, type="prob")[,"Yes"]
plot(rf_rocCurve, print.thres=TRUE, print.auc=TRUE)
pi_hat <- predict(final_forest, rf_test, type="prob")[,"1"]
rf_train <- train.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS_fact = as.factor(train.df$FSFOODS)
)
rf_test <- test.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS_fact = as.factor(test.df$FSFOODS)
)
rf_train <- train.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS_fact = as.factor(train.df$FSFOODS)
)
rf_test <- test.df %>% select(-c("FSFOODS")) %>%
mutate(
FSFOODS_fact = as.factor(test.df$FSFOODS)
)
rf_init_fsfoods <- randomForest(FSFOODS_fact ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
mtry=floor(sqrt(length(x_vars))),
ntree=1000,
importance=TRUE)
#Multiple mtry
mtry = c(1:length(x_vars))
keeps <- data.frame(m=rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for (idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSFOODS_fact ~hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != rf_train$FSFOODS_fact) #Estimates out of sample error
}
best_m <- keeps[order(keeps$OOB_err_rate),"m"][1]
final_forest <- randomForest(FSFOODS_fact ~ hhsize + married + education + elderly +
kids + black + hispanic + female + faminc_cleaned,
data=rf_train,
mtry=best_m,
ntree=1000,
importance=TRUE)
pi_hat <- predict(final_forest, rf_test, type="prob")[,"1"]
rf_rocCurve <- roc(response=rf_test$FSSTMPVALC_bin_fact,
predictor=pi_hat,
levels=c("No", "Yes"))
rf_rocCurve <- roc(response=rf_test$FSFOODS_fact,
predictor=pi_hat,
levels=c("No", "Yes"))
rf_rocCurve <- roc(response=rf_test$FSFOODS_fact,
predictor=pi_hat,
levels=c("0", "1"))
plot(rf_rocCurve, print.thres=TRUE, print.auc=TRUE)
rf_fsstmp_pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test.df.preds <- test.df.preds %>% mutate(
rf_fsstmp_preds = as.factor(ifelse(pi_hat > rf_fsstmp_pi_star, "Yes", "No"))
)
varImpPlot(final_forest, type=1)
rf_vi <- as.data.frame(varImpPlot(final_forest, type=1))
rf_vi$Variable <- rownames(rf_vi)
rf_vi <- rf_vi %>% arrange(desc(MeanDecreaseAccuracy))
rf_pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
rf_pi_star
#----Compare Models' Performances----
#First, on the testing data split
test.df.cpspreds <- test.df %>%
mutate(
mle_pred = predict(lr_mle_fsfoods, test.df, type = "response"),
fmle_pred = predict(lr_fmle_fsfoods, test.df, type = "response"),
lasso_pred = predict(fsfoods_lasso_f1, fsfoods.x.test, type = "response")[,1],
ridge_pred = predict(fsfoods_ridge_f1, fsfoods.x.test, type = "response")[,1],
rf_pi_hat <- predict(final_forest, rf_test, type="prob")[,"1"]
)
mle_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$mle_pred,
levels = c("0", "1"))
fmle_rocCurve<- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$fmle_pred,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$lasso_pred,
levels = c("0", "1"))
ridge_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$ridge_pred,
levels = c("0", "1"))
rf_rocCurve <- roc(response=rf_test$FSFOODS,
predictor=rf_pi_hat,
levels=c("0", "1"))
#Fit ROC Curves on CPS
mle_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$mle_pred,
levels = c("0", "1"))
fmle_rocCurve<- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$fmle_pred,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$lasso_pred,
levels = c("0", "1"))
ridge_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$ridge_pred,
levels = c("0", "1"))
rf_rocCurve <- roc(response=rf_test$FSFOODS_fact,
predictor=pi_hat,
levels=c("0", "1"))
#PLOT CPS PREDICTIONS
#make data frame of MLE ROC info
mle_data <- data.frame(
Model = "MLE",
Specificity = mle_rocCurve$specificities,
Sensitivity = mle_rocCurve$sensitivities,
AUC = as.numeric(mle_rocCurve$auc)
)
#make data frame of Firth's ROC info
fmle_data<- data.frame(
Model = "Firth's",
Specificity = fmle_rocCurve$specificities,
Sensitivity = fmle_rocCurve$sensitivities,
AUC = as.numeric(fmle_rocCurve$auc)
)
#make data frame of lasso ROC info
lasso_data <- data.frame(
=======
tempforest <- randomForest(as.factor(FSWROUTY_bin) ~.,
data= train.df,
ntree=1000,
mtry=mtry[idx])
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$FSWROUTY_bin) #Estimates out of sample error
}
best_m <- keeps[order(keeps$OOB_err_rate),"m"][1]
library(ggplot2)
ggplot(keeps, aes(x = m, y = OOB_err_rate)) +
geom_line() +
geom_point() +
labs(title = "OOB Error Rate vs mtry",
x = "mtry",
y = "OOB Error Rate") +
theme_minimal()
# final RF model
rf_fswrouty <- randomForest(as.factor(FSWROUTY_bin) ~ .,
data = train.df,
ntree = 1000,
mtry = best_m,
importance = TRUE)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
rf_rocCurve <- roc(response = test.df$FSWROUTY_bin,
predictor = pi_hat,
levels = c("0", "1"))
plot(rf_rocCurve, print.thres = TRUE, print.ouc = TRUE)
auc(rf_rocCurve)
pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test_preds <- test.df %>% mutate(
rf_preds = as.factor(ifelse(pi_hat > pi_star, "Yes", "No"))
)
varImpPlot(rf_fswrouty, type=1)
rf_vi <- as.data.frame(varImpPlot(rf_fswrouty, type=1))
rf_vi$Variable <- rownames(rf_vi)
rf_vi <- rf_vi %>% arrange(desc(MeanDecreaseAccuracy))
################ Tree #####################
ctree <- rpart(FSWROUTY_bin ~ .,
data= train.df,
method="class",
control=rpart.control(cp=.0001, minsplit=1))
optimalcp <- ctree$cptable[which.min(ctree$cptable[,"xerror"]), "CP"]
ctree_optimal <- prune(ctree, cp=optimalcp)
rpart.plot(ctree_optimal)
pi_hat <- predict(ctree_optimal, test.df, type="prob")[,"1"]
ctree_rocCurve <- roc(response=test.df$FSWROUTY_bin,
predictor=pi_hat,
levels=c("0", "1"))
plot(ctree_rocCurve, print.thres=TRUE, print.auc=TRUE)
ctree_pi_star <- coords(rf_rocCurve, "best", ret="threshold")$threshold[1]
test_preds <- test_preds %>% mutate(
ctree_preds = as.factor(ifelse(pi_hat > ctree_pi_star, "Yes", "No"))
)
###########################################
x_train <- model.matrix(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female + faminc_cleaned
, data=train.df)[,-1]
x_test <- model.matrix(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female + faminc_cleaned
, data=test.df)[,-1]
# y_train <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
y_test <- as.vector(test.df$FSWROUTY_bin)
y_train <- as.vector(train.df$FSWROUTY_bin)
########### LASSO ##################
fswrouty_lasso <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# cluster 2, 4, and 5 went to 0
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 1,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
########### RIDGE ##############
fswrouty_ridge <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 0)
plot(fswrouty_ridge)
best_lambda_ridge <- fswrouty_ridge$lambda.min
coef(fswrouty_ridge, s="lambda.min") %>% as.matrix()
ridge_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight))
# predict probability on the test set
test_preds <- test.df %>%
mutate (
ridge_prob = predict(ridge_model, x_test, type = "response"))
ridge_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$ridge_prob,
levels = c("0", "1"))
plot(ridge_rocCurve, print.thres=TRUE, print.auc=TRUE)
########## COMBINE ROC CURVE ################
#make data frame of MLE ROC info
mle_data_fswrouty <- data.frame(
Model = "MLE",
Specificity = mle_fswrouty_rocCurve$specificities,
Sensitivity = mle_fswrouty_rocCurve$sensitivities,
AUC = as.numeric(mle_fswrouty_rocCurve$auc)
)
#make data frame of Firths ROC info
firths_data_fswrouty <- data.frame(
Model = "Firths",
Specificity = firths_fswrouty_rocCurve$specificities,
Sensitivity = firths_fswrouty_rocCurve$sensitivities,
AUC = as.numeric(firths_fswrouty_rocCurve$auc)
)
#make data frame of lasso ROC info
lasso_data_fswrouty <- data.frame(
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
Model = "Lasso",
Specificity = lasso_rocCurve$specificities,
Sensitivity = lasso_rocCurve$sensitivities,
AUC = lasso_rocCurve$auc %>% as.numeric
)
#make data frame of ridge ROC info
<<<<<<< HEAD
ridge_data <- data.frame(
=======
ridge_data_fswrouty <- data.frame(
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
Model = "Ridge",
Specificity = ridge_rocCurve$specificities,
Sensitivity = ridge_rocCurve$sensitivities,
AUC = ridge_rocCurve$auc%>% as.numeric
)
<<<<<<< HEAD
#make data frame of forest ROC info
rf_data <- data.frame(
Model = "Forest",
Specificity = rf_rocCurve$specificities,
Sensitivity = rf_rocCurve$sensitivities,
AUC = rf_rocCurve$auc%>% as.numeric
)
# Combine all the data frames
roc_data <- rbind(mle_data, fmle_data, lasso_data, ridge_data, forest_data)
# Combine all the data frames
roc_data <- rbind(mle_data, fmle_data, lasso_data, ridge_data, rf_data)
=======
# make data frame of ctree ROC info
ctree_fswrouty <- data.frame(
Model = "Categorical Tree",
Specificity = ctree_rocCurve$specificities,
Sensitivity = ctree_rocCurve$sensitivities,
AUC = ctree_rocCurve$auc %>% as.numeric
)
rf_fswrouty <- data.frame(
Model = "Random Forest",
Specificity = rf_rocCurve$specificities,
Sensitivity = rf_rocCurve$sensitivities,
AUC = rf_rocCurve$auc %>% as.numeric
)
# Combine all the data frames
roc_data <- rbind(firths_data_fswrouty, lasso_data_fswrouty,
ridge_data_fswrouty, rf_fswrouty, ctree_fswrouty)
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
# Plot the data
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
<<<<<<< HEAD
aes(x = 0.75, y = c(0.85, 0.75, 0.65, 0.55, 0.45), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 3)))) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
#----Random Tree----
#Create big tree, then prune
set.seed(123432456)
ctree <- rpart(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female+ faminc_cleaned,
data = train.df, #training data, NOT original data
method = "class",
control = rpart.control(cp = 0.0001, minsplit = 1))
printcp(ctree)
optimalcp <- ctree$cptable[which.min(ctree$cptable[,"xerror"]),"CP"]
#gives you the optimal complexity parameter (cp of tree with smallest xerror)
#prune to create tuned tree
ctree2 <- prune(ctree, cp = optimalcp)
rpart.plot(ctree2)
optimalcp
#----Random Tree----
#Create big tree, then prune
set.seed(578493768)
ctree <- rpart(FSFOODS ~ hhsize + married + education + elderly +
kids + black + hispanic + female+ faminc_cleaned,
data = train.df, #training data, NOT original data
method = "class",
control = rpart.control(cp = 0.0001, minsplit = 1))
printcp(ctree)
optimalcp <- ctree$cptable[which.min(ctree$cptable[,"xerror"]),"CP"]
#gives you the optimal complexity parameter (cp of tree with smallest xerror)
#prune to create tuned tree
ctree2 <- prune(ctree, cp = optimalcp)
rpart.plot(ctree2)
#----Compare Models' Performances----
#First, on the testing data split
test.df.cpspreds <- test.df %>%
mutate(
mle_pred = predict(lr_mle_fsfoods, test.df, type = "response"),
fmle_pred = predict(lr_fmle_fsfoods, test.df, type = "response"),
lasso_pred = predict(fsfoods_lasso_f1, fsfoods.x.test, type = "response")[,1],
ridge_pred = predict(fsfoods_ridge_f1, fsfoods.x.test, type = "response")[,1],
rf_pi_hat = predict(final_forest, rf_test, type="prob")[,"1"],
rt_pi_hat = predict(ctree2, test.df, type = "prob")[,"1"]
)
rf_rocCurve <- roc(response=rf_test$FSFOODS_fact,
predictor=test.df.cpspreds$rf_pi_hat,
levels=c("0", "1"))
rt_rocCurve <- roc(response = as.factor(test.df.cpspreds$FSFOODS),
predictor = test.df.cpspreds$rt_pi_hat,
levels = c("0", "1"))
#make data frame of forest ROC info
rf_data <- data.frame(
Model = "Forest",
Specificity = rf_rocCurve$specificities,
Sensitivity = rf_rocCurve$sensitivities,
AUC = rf_rocCurve$auc%>% as.numeric
)
#make data frame of tree ROC info
rt_data <- data.frame(
Model = "Tree",
Specificity = rt_rocCurve$specificities,
Sensitivity = rt_rocCurve$sensitivities,
AUC = rt_rocCurve$auc%>% as.numeric
)
# Combine all the data frames
roc_data <- rbind(mle_data, fmle_data, lasso_data, ridge_data, rf_data, rt_data)
# Plot the data
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
aes(x = 0.75, y = c(0.85, 0.75, 0.65, 0.55, 0.45), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 3)))) +
=======
aes(x = 0.75, y = c(0.75, 0.65, 0.55, 0.45), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 5)))) +
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
# Plot the data
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
<<<<<<< HEAD
aes(x = 0.75, y = c(0.85, 0.75, 0.65, 0.55, 0.45, 0.35), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 3)))) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
=======
aes(x = 0.75, y = c(0.75, 0.65, 0.55, 0.45, 0.35), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 5)))) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female + faminc_cleaned,
#data = train.df,
data = reduced_train
family = binomial(link = 'logit'),
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female + faminc_cleaned,
#data = train.df,
data = reduced_train,
family = binomial(link = 'logit'),
weights = weight)
include_squared_interaction = FALSE
#With or without interactions/squared terms
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 <- colnames(train.df)[i][1]
col2 <- colnames(train.df)[j][1]
col_str <- paste(col1, col2, sep="_")
reduced_train = train.df %>%
mutate(interaction_term = (train.df[col1] * train.df[col2])[,1])
reduced_test = test.df %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
}
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female + faminc_cleaned,
#data = train.df,
data = reduced_train,
family = binomial(link = 'logit'),
weights = weight)
colnames(reduced_train)
include_squared_interaction = FALSE
#With or without interactions/squared terms
if(include_squared_interaction){
for(i in 1:length(x_vars)){
for (j in i:length(x_vars)){
col1 <- colnames(train.df)[i][1]
col2 <- colnames(train.df)[j][1]
col_str <- paste(col1, col2, sep="_")
reduced_train = train.df %>%
mutate(interaction_term = (train.df[col1] * train.df[col2])[,1])
reduced_test = test.df %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
}
colnames(reduced_train)
>>>>>>> de3c0cc8d0a515360f7376957edb3df8079cc3a2
