test_pred_probs2 <- predict(fswrouty_mle2, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
fswrouty_mle2 <-  glm(FSWROUTY_bin ~ weights + hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
table(train.df$FSWROUTY_bin)
library(caret)
nearZeroVar(train.df[, c("h_cluster", "weight", "hhsize", "married", "education",
"elderly", "kids", "black", "hispanic", "female")], saveMetrics = TRUE)
fswrouty_mle2 <- glm(FSWROUTY_bin ~ weight +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle2)
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
# Evaluate Model Accuracy
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
rf_fswrouty <- randomForest(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
importance(rf_fswrouty)
varImpPlot(rf_fswrouty)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "Yes"] #Choose positive event column
rocCurve <- roc(response = test.df$FSWROUTY_bin,
predictor = pi_hat,
levels = c("No", "Yes"))
plot(rocCurve, print.thres = TRUE, print.ouc = TRUE)
auc(rocCurve)
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df[, -1])
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df[, -1])
train.df$FSWROUTY_bin <- as.factor(train.df$FSWROUTY_bin)
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df[, -1])
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df)
x
y <- as.numeric(train.df$FSWROUTY_bin) - 1
cv.lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1)
best.lambda <- cv.lasso$lambda.min
lasso_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best.lambda)
lasso_model
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
best_lambda <- fswrouty_lasso$lambda.min
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
x <- model.matrix(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female . -1
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.vector(train.df$FSWROUTY_bin)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
install.packages("logistf")
library(logistf)
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
# Evaluate Model Accuracy
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
lasso_model <- glmnet(x ,y,family = binomial(link ="logit"), lambda = best_lambda_lasso)
lasso_model
summary(lasso_model)
str(lasso_model)
fswrouty_ridge <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 0)
plot(fswrouty_ridge)
firths_fswrouty <- logistf(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
# Evaluate Model Accuracy
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = weight
)
x <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
########### LASSO ##################
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# hhsize went to 0
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = weight
)
x <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
y <- as.vector(train.df$FSWROUTY_bin) -1
x <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
########### LASSO ##################
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = weight
)
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
lasso_prob <- predict(lasso_model, x, type = "response")[,1]
lasso_prob
lasso_prob <- lasso_prob %>% mutate (lasso_model)
lasso_model <- lasso_model %>% mutate (lasso_prob)
lasso_model <- lasso_model %>% mutate (lasso_model = predict(lasso_model, x, type = "response")[,1])
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
lasso_model <- lasso_model %>% mutate (lasso_model = predict(lasso_model, x, type = "response")[,1])
lasso_model <- lasso_model %>% mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
test_preds <- test_preds %>%
mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
test_preds <- test.df
test_preds <- test_preds %>%
mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
lasso_rocCurve <- roc(response = as.factor(lasso_model$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
test_preds <- test.df %>%
mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
gc()
rm(list=ls())
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
source("code/clean_cps.R")
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
# cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = ifelse(FSWROUTY_binchar == "No", 0, 1))
summary(cps_data)
head(cps_data)
# set up
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
# Add cluster membership back to the main dataset
cps_data$h_cluster <- as.factor(cutree(data_clust, k = 5))
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin", "h_cluster", "weight", "hhsize", "female",
"hispanic", "black", "kids", "elderly", "education", "married")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
# Evaluate Model Accuracy
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
test_pred_classes <- ifelse(test_pred_probs > 0.5, 1, 0)
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
# test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "1", "0")
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
source("code/clean_cps.R")
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
# cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = ifelse(FSWROUTY_binchar == "No", 0, 1))
summary(cps_data)
head(cps_data)
#######################################
############# CLUSTERING ##############
#######################################
# set up
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
# Cluster 1: small households, elderly or retired individuals living alone or with a spouse
#           => might benefit us to look more into this cluster
# Cluster 2: young black families
# Cluster 3: large families with children
# Cluster 4: medium size Hispanic families
#           => big family size with children and elderly
# Cluster 5: Outliers or Minimal Engagement
############ Train Test Split ############
# Add cluster membership back to the main dataset
cps_data$h_cluster <- as.factor(cutree(data_clust, k = 5))
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin", "h_cluster", "weight", "hhsize", "female",
"hispanic", "black", "kids", "elderly", "education", "married")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
################ MLE #########################
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
# weight has a strong positive effect: Higher weights increase the likelihood of "Yes."
# education and elderly have negative effects
# black and hispanic have positive effects
# Evaluate Model Accuracy
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
# test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "1", "0")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
# Firth's Penalized Likelihood #
firths_fswrouty <- logistf(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
# Evaluate Model Accuracy
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
x_train <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
x_test <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=test.df)[,-1]
# y_train <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
y_test <- as.vector(test.df$FSWROUTY_bin)
y_train <- as.vector(train.df$FSWROUTY_bin)
fswrouty_lasso <- cv.glmnet(x_test, y_test, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = train.df$weight
)
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(lasso_model$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
fswrouty_lasso <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# cluster 2, 4, and 5 went to 0
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
fswrouty_lasso <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# cluster 2, 4, and 5 went to 0
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 1,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
fswrouty_ridge <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 0)
plot(fswrouty_ridge)
best_lambda_ridge <- fswrouty_ridge$lambda.min
coef(fswrouty_ridge, s="lambda.min") %>% as.matrix()
ridge_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight))
# predict probability on the test set
test_preds <- test.df %>%
mutate (
ridge_prob = predict(ridge_model, x_test, type = "response"))
ridge_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$ridge_prob,
levels = c("0", "1"))
plot(ridge_rocCurve, print.thres=TRUE, print.auc=TRUE)
