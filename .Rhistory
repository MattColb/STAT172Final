<<<<<<< HEAD
test_pred_probs2 <- predict(fswrouty_mle2, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
fswrouty_mle2 <-  glm(FSWROUTY_bin ~ weights + hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
table(train.df$FSWROUTY_bin)
library(caret)
nearZeroVar(train.df[, c("h_cluster", "weight", "hhsize", "married", "education",
"elderly", "kids", "black", "hispanic", "female")], saveMetrics = TRUE)
fswrouty_mle2 <- glm(FSWROUTY_bin ~ weight +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle2)
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
# Evaluate Model Accuracy
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
rf_fswrouty <- randomForest(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female ,
=======
#########################################
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.test <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSWROUTY.y.train <- as.vector(train.df$FSWROUTY)
FSWROUTY.y.test <- as.vector(test.df$FSWROUTY)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
lr_lasso <- cv.glmnet(FSWROUTY.x.train, FSWROUTY.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
plot(lr_lasso)
best_lasso_lambda <- lr_lasso$lambda.min
best_lasso_lambda
lr_lasso_coefs <- coef(lr_lasso, s="lambda.min") %>% as.matrix()
lr_lasso_coefs <- coef(lr_lasso, s="lambda.min") %>% as.matrix()
lr_lasso_coefs
lr_lasso <- glmnet(FSSTMP.x.train, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lr_lasso <- glmnet(FSWROUTY.x.train, FSWROUTY.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda, weights=train.weights)
lr_lasso <- glmnet(FSWROUTY.x.train, FSWROUTY.y.train, family = "multinomial",
alpha=1, lambda = best_lasso_lambda, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_FSWROUTY_preds = predict(lr_lasso, FSWROUTY.x.test, type="response")[,1]
)
test.df.preds <- test.df.preds %>% mutate(
lasso_FSWROUTY_preds = predict(lr_lasso, FSWROUTY.x.test, type="response"))
lasso_FSWROUTY_rocCurve <- roc(response = as.factor(test.df.preds$FSWROUTY),
predictor =test.df.preds$lasso_FSWROUTY_preds,
levels=c("0", "1"))
lasso_FSWROUTY_rocCurve <- roc(response = as.factor(test.df.preds$FSWROUTY),
predictor =test.df.preds$lasso_FSWROUTY_preds)
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
cps_data <- source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
columns(cps_data)
column(cps_data)
colnames(cps_data)
summary(cps_data)
head(cps_data)
cps_data <- source("code/clean_cps.R")
summary(cps_data)
source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
str(data_X)
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
cps_data$county <- as.character(cps_data$county)
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
summary(cps_data)
str(cps_data)
data_X <- cps_data[, c() "weight", "hhsize", "female", "hispanic", "black",
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure clsuter-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.test <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSWROUTY.y.train <- as.vector(train.df$FSWROUTY)
FSWROUTY.y.test <- as.vector(test.df$FSWROUTY)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
tempforest <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 4)
dim(train.df)
tempforest <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 5)
dim(train.df)
mtry <- seq(from = 1, to = 21, by = 3) # to slim down number of tuning options
keeps <- data.frame(m = rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY.x.train,
data = train.df,
ntree= 1000,
mtry = mtry[idx])
# record iteration's m value in idx th row
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$luxury)
}
mtry <- seq(from = 1, to = 21, by = 3) # to slim down number of tuning options
keeps <- data.frame(m = rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY.x.train,
data = train.df,
ntree= 1000,
mtry = mtry[idx])
# record iteration's m value in idx th row
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$luxury)
}
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
str(cps_data)
str(train.df)
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.train
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
rf_fswrouty <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 5,
importance = TRUE)
dim(train.df)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")
pi_hat <- predict(rf_fswrouty, FSWROUTY.x.test, type = "prob")[, "1"]
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 5,
importance = TRUE)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
>>>>>>> 11ab1e7a676e951327456630641db087c1620ba0
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
importance(rf_fswrouty)
varImpPlot(rf_fswrouty)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "Yes"] #Choose positive event column
rocCurve <- roc(response = test.df$FSWROUTY_bin,
predictor = pi_hat,
levels = c("No", "Yes"))
plot(rocCurve, print.thres = TRUE, print.ouc = TRUE)
auc(rocCurve)
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df[, -1])
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df[, -1])
train.df$FSWROUTY_bin <- as.factor(train.df$FSWROUTY_bin)
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df[, -1])
x <- model.matrix(FSWROUTY_bin ~ . -1, data = train.df)
x
y <- as.numeric(train.df$FSWROUTY_bin) - 1
cv.lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1)
best.lambda <- cv.lasso$lambda.min
lasso_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best.lambda)
lasso_model
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
best_lambda <- fswrouty_lasso$lambda.min
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
x <- model.matrix(FSWROUTY_bin ~ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1, weights = weight)
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female . -1
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.vector(train.df$FSWROUTY_bin)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
install.packages("logistf")
library(logistf)
firths_fswrouty <- logistf(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
# Evaluate Model Accuracy
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
x <- model.matrix(FSWROUTY_bin ~ weight+ hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
# LASSO
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
lasso_model <- glmnet(x ,y,family = binomial(link ="logit"), lambda = best_lambda_lasso)
lasso_model
summary(lasso_model)
str(lasso_model)
fswrouty_ridge <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 0)
plot(fswrouty_ridge)
firths_fswrouty <- logistf(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly +
kids + black + hispanic + female,
data=train.df,
weights=weight)
summary(firths_fswrouty)
# Evaluate Model Accuracy
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = weight
)
x <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
########### LASSO ##################
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# hhsize went to 0
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = weight
)
x <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.vector(train.df$FSWROUTY_bin)
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
y <- as.vector(train.df$FSWROUTY_bin) -1
x <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
y <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
########### LASSO ##################
fswrouty_lasso <- cv.glmnet(x, y, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = weight
)
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
lasso_prob <- predict(lasso_model, x, type = "response")[,1]
lasso_prob
lasso_prob <- lasso_prob %>% mutate (lasso_model)
lasso_model <- lasso_model %>% mutate (lasso_prob)
lasso_model <- lasso_model %>% mutate (lasso_model = predict(lasso_model, x, type = "response")[,1])
lasso_model <- glmnet(x, y, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
lasso_model <- lasso_model %>% mutate (lasso_model = predict(lasso_model, x, type = "response")[,1])
lasso_model <- lasso_model %>% mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
test_preds <- test_preds %>%
mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
test_preds <- test.df
test_preds <- test_preds %>%
mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
lasso_rocCurve <- roc(response = as.factor(lasso_model$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
test_preds <- test.df %>%
mutate (lasso_prob = predict(lasso_model, x, type = "response")[,1])
gc()
rm(list=ls())
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
source("code/clean_cps.R")
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
# cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = ifelse(FSWROUTY_binchar == "No", 0, 1))
summary(cps_data)
head(cps_data)
# set up
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
# Add cluster membership back to the main dataset
cps_data$h_cluster <- as.factor(cutree(data_clust, k = 5))
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin", "h_cluster", "weight", "hhsize", "female",
"hispanic", "black", "kids", "elderly", "education", "married")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
# Evaluate Model Accuracy
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
test_pred_classes <- ifelse(test_pred_probs > 0.5, 1, 0)
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
# test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "1", "0")
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
library(logistf)
source("code/clean_cps.R")
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
# cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = ifelse(FSWROUTY_binchar == "No", 0, 1))
summary(cps_data)
head(cps_data)
#######################################
############# CLUSTERING ##############
#######################################
# set up
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
# Cluster 1: small households, elderly or retired individuals living alone or with a spouse
#           => might benefit us to look more into this cluster
# Cluster 2: young black families
# Cluster 3: large families with children
# Cluster 4: medium size Hispanic families
#           => big family size with children and elderly
# Cluster 5: Outliers or Minimal Engagement
############ Train Test Split ############
# Add cluster membership back to the main dataset
cps_data$h_cluster <- as.factor(cutree(data_clust, k = 5))
# Combine cluster memberships with other features
model_data <- cps_data[, c("FSWROUTY_bin", "h_cluster", "weight", "hhsize", "female",
"hispanic", "black", "kids", "elderly", "education", "married")]
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(model_data), size=.7*nrow(model_data))
train.df <- model_data[train.idx,]
test.df <- model_data[-train.idx,]
################ MLE #########################
fswrouty_mle <- glm(FSWROUTY_bin ~ h_cluster +hhsize + married +
education + elderly + kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'),
weights = weight)
summary(fswrouty_mle)
# weight has a strong positive effect: Higher weights increase the likelihood of "Yes."
# education and elderly have negative effects
# black and hispanic have positive effects
# Evaluate Model Accuracy
test_pred_probs <- predict(fswrouty_mle, newdata = test.df, type = "response")
# test_pred_classes <- ifelse(test_pred_probs > 0.5, "Yes", "No")
test_pred_classes <- ifelse(test_pred_probs > 0.5, "1", "0")
library(caret)
confusionMatrix(as.factor(test_pred_classes), test.df$FSWROUTY_bin)
# Firth's Penalized Likelihood #
firths_fswrouty <- logistf(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly +
kids + black + hispanic + female,
<<<<<<< HEAD
data=train.df,
weights=weight)
summary(firths_fswrouty)
# Evaluate Model Accuracy
test_pred_probs2 <- predict(firths_fswrouty, newdata = test.df, type = "response")
test_pred_classes2 <- ifelse(test_pred_probs2 > 0.5, "Yes", "No")
library(caret)
confusionMatrix(as.factor(test_pred_classes2), test.df$FSWROUTY_bin)
x_train <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=train.df)[,-1]
x_test <- model.matrix(FSWROUTY_bin ~ h_cluster + hhsize + married + education + elderly + kids
+ black + hispanic + female
, data=test.df)[,-1]
# y_train <- as.numeric(train.df$FSWROUTY_bin) - 1  # Convert factor (1, 2) to binary (0, 1)
y_test <- as.vector(test.df$FSWROUTY_bin)
y_train <- as.vector(train.df$FSWROUTY_bin)
fswrouty_lasso <- cv.glmnet(x_test, y_test, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = train.df$weight
)
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(lasso_model$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
fswrouty_lasso <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# cluster 2, 4, and 5 went to 0
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
fswrouty_lasso <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 1)
plot(fswrouty_lasso)
best_lambda_lasso <- fswrouty_lasso$lambda.min
coef(fswrouty_lasso, s="lambda.min") %>% as.matrix()
# cluster 2, 4, and 5 went to 0
lasso_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 1,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight)
)
# predict probability on the test set
test_preds <- test.df %>%
mutate (
lasso_prob = predict(lasso_model, x_test, type = "response"))
lasso_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$lasso_prob,
levels = c("0", "1"))
plot(lasso_rocCurve, print.thres=TRUE, print.auc=TRUE)
fswrouty_ridge <- cv.glmnet(x_train, y_train, family=binomial(link="logit"), alpha = 0)
plot(fswrouty_ridge)
best_lambda_ridge <- fswrouty_ridge$lambda.min
coef(fswrouty_ridge, s="lambda.min") %>% as.matrix()
ridge_model <- glmnet(x_train, y_train, family=binomial(link="logit"),
alpha = 0,
lambda = best_lambda_lasso,
weights = as.vector(train.df$weight))
# predict probability on the test set
test_preds <- test.df %>%
mutate (
ridge_prob = predict(ridge_model, x_test, type = "response"))
ridge_rocCurve <- roc(response = as.factor(test_preds$FSWROUTY_bin),
predictor = test_preds$ridge_prob,
levels = c("0", "1"))
plot(ridge_rocCurve, print.thres=TRUE, print.auc=TRUE)
=======
data = train.df,
family = binomial(link = 'logit'))
warnings()
beta <- fswrouty_mle %>% coef()
beta
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'), weights = weight)
>>>>>>> Stashed changes
source("C:/Users/mattc/Desktop/FA24/STAT172Final/code/fsstamp_analysis.R")
str(reduced_train)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
str(reduced_train)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in i:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
str(reduced_test)
reduced_train = train.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:8){
for (j in i:8){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
reduced_train_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_train)[,-1]
reduced_test_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_test)[,-1]
reduced_train_y <- as.vector(reduced_train$FSSTMPVALC_bin)
reduced_test_y <- as.vector(reduced_test$FSSTMPVALC_bin)
################################################
# Making Lasso with Interaction/Squared Terms  #
################################################
lr_lasso_fsstmp_cv_2 <- cv.glmnet(reduced_train_matrix_x, reduced_train_y,
family=binomial(link="logit"), alpha=1, weights=train.weights)
best_lasso_lambda_fsstmp_2 <- lr_lasso_fsstmp_cv_2$lambda.min
lr_lasso_fsstmp_2 <- glmnet(reduced_train_x, reduced_train_y, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp_2, weights=train.weights)
lr_lasso_fsstmp_2 <- glmnet(reduced_train_matrix_x, reduced_train_y, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp_2, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_fsstmp_preds_2 = predict(lr_lasso_fsstmp_2, reduced_test_matrix_x, type="response")[,1]
)
lasso_fsstmp_rocCurve <- roc(response = as.factor(test.df.preds$FSSTMPVALC_bin),
predictor =test.df.preds$lasso_fsstmp_preds_2,
levels=c("0", "1"))
plot(lasso_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #Better at AUC = .798, (.681, .810)
lasso_fsstmp_pi_star <- coords(lasso_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
lr_lass_fsstmp_2
summary(lr_lasso_fsstmp_2)
str(reduced_train)
reduced_train_matrix_x
coef(lr_lasso_fsstmp_2)
reduced_train = train.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:8){
for (j in i:8){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
reduced_train_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_train)[,-1]
reduced_test_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_test)[,-1]
reduced_train_y <- as.vector(reduced_train$FSSTMPVALC_bin)
reduced_test_y <- as.vector(reduced_test$FSSTMPVALC_bin)
################################################
# Making Lasso with Interaction/Squared Terms  #
################################################
lr_lasso_fsstmp_cv_2 <- cv.glmnet(reduced_train_matrix_x, reduced_train_y,
family=binomial(link="logit"), alpha=0, weights=train.weights)
best_lasso_lambda_fsstmp_2 <- lr_lasso_fsstmp_cv_2$lambda.min
lr_lasso_fsstmp_2 <- glmnet(reduced_train_matrix_x, reduced_train_y, family=binomial(link="logit"),
alpha=0, lambda = best_lasso_lambda_fsstmp_2, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_fsstmp_preds_2 = predict(lr_lasso_fsstmp_2, reduced_test_matrix_x, type="response")[,1]
)
lasso_fsstmp_rocCurve <- roc(response = as.factor(test.df.preds$FSSTMPVALC_bin),
predictor =test.df.preds$lasso_fsstmp_preds_2,
levels=c("0", "1"))
plot(lasso_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #Better at AUC = .798, (.681, .810)
lasso_fsstmp_pi_star <- coords(lasso_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
warnings()
reduced_train_y
unique(reduced_train_y)
>>>>>>> 11ab1e7a676e951327456630641db087c1620ba0
