#########################################
# splitting training and testing
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.test <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSWROUTY.y.train <- as.vector(train.df$FSWROUTY)
FSWROUTY.y.test <- as.vector(test.df$FSWROUTY)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
lr_lasso <- cv.glmnet(FSWROUTY.x.train, FSWROUTY.y.train,
family=binomial(link="logit"), alpha=1, weights=train.weights)
plot(lr_lasso)
best_lasso_lambda <- lr_lasso$lambda.min
best_lasso_lambda
lr_lasso_coefs <- coef(lr_lasso, s="lambda.min") %>% as.matrix()
lr_lasso_coefs <- coef(lr_lasso, s="lambda.min") %>% as.matrix()
lr_lasso_coefs
lr_lasso <- glmnet(FSSTMP.x.train, FSSTMP.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp, weights=train.weights)
lr_lasso <- glmnet(FSWROUTY.x.train, FSWROUTY.y.train, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda, weights=train.weights)
lr_lasso <- glmnet(FSWROUTY.x.train, FSWROUTY.y.train, family = "multinomial",
alpha=1, lambda = best_lasso_lambda, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_FSWROUTY_preds = predict(lr_lasso, FSWROUTY.x.test, type="response")[,1]
)
test.df.preds <- test.df.preds %>% mutate(
lasso_FSWROUTY_preds = predict(lr_lasso, FSWROUTY.x.test, type="response"))
lasso_FSWROUTY_rocCurve <- roc(response = as.factor(test.df.preds$FSWROUTY),
predictor =test.df.preds$lasso_FSWROUTY_preds,
levels=c("0", "1"))
lasso_FSWROUTY_rocCurve <- roc(response = as.factor(test.df.preds$FSWROUTY),
predictor =test.df.preds$lasso_FSWROUTY_preds)
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
cps_data <- source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# set up
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
columns(cps_data)
column(cps_data)
colnames(cps_data)
summary(cps_data)
head(cps_data)
cps_data <- source("code/clean_cps.R")
summary(cps_data)
source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
data_X <- cps_data[, c("county", "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
str(data_X)
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
cps_data$county <- as.character(cps_data$county)
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
summary(cps_data)
str(cps_data)
data_X <- cps_data[, c() "weight", "hhsize", "female", "hispanic", "black",
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure clsuter-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
# handle missing values
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
test.df.preds <- test.df
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.test <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=test.df)[,-1]
FSWROUTY.y.train <- as.vector(train.df$FSWROUTY)
FSWROUTY.y.test <- as.vector(test.df$FSWROUTY)
train.weights <- as.vector(train.df$weight)
test.weights <- as.vector(test.df$weight)
tempforest <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 4)
dim(train.df)
tempforest <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 5)
dim(train.df)
mtry <- seq(from = 1, to = 21, by = 3) # to slim down number of tuning options
keeps <- data.frame(m = rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY.x.train,
data = train.df,
ntree= 1000,
mtry = mtry[idx])
# record iteration's m value in idx th row
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$luxury)
}
mtry <- seq(from = 1, to = 21, by = 3) # to slim down number of tuning options
keeps <- data.frame(m = rep(NA, length(mtry)),
OOB_err_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
print(paste0("Trying m = ", mtry[idx]))
tempforest <- randomForest(FSWROUTY.x.train,
data = train.df,
ntree= 1000,
mtry = mtry[idx])
# record iteration's m value in idx th row
keeps[idx, "m"] <- mtry[idx]
keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$luxury)
}
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
source("code/clean_cps.R")
summary(cps_data)
head(cps_data)
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
train.df$FSWROUTY[is.na(train.df$FSWROUTY)] <- median(train.df$FSWROUTY, na.rm = TRUE)
test.df$FSWROUTY[is.na(test.df$FSWROUTY)] <- median(test.df$FSWROUTY, na.rm = TRUE)
str(cps_data)
str(train.df)
FSWROUTY.x.train <- model.matrix(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female
, data=train.df)[,-1]
FSWROUTY.x.train
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
rf_fswrouty <- randomForest(FSWROUTY.x.train,
ntree = 1000,
mtry = 5,
importance = TRUE)
dim(train.df)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")
pi_hat <- predict(rf_fswrouty, FSWROUTY.x.test, type = "prob")[, "1"]
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 5,
importance = TRUE)
# Validate model as predictive tool
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"]
cps_data$FSWROUTY <- ifelse(0, "No", "Yes")
cps_data$FSWROUTY_bin <- ifelse(0, "No", "Yes")
cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_bin))
summary(cps_data)
cps_data$FSWROUTY_bin <- ifelse(cps_data$FSWROTY == 0, "No", "Yes")
# change y-variable into factor
cps_data$FSWROUTY_bin <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
cps_data$FSWROUTY_bin
summary(cps_data)
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
summary(cps_data)
gc()
rm(list=ls())
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
source("code/clean_cps.R")
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
summary(cps_data)
# handle missing values
cps_data$FSWROUTY_binchar <- na.omit(cps_data$FSWROUTY_binchar)
# handle missing values
cps_data$FSWROUTY_bin <- na.omit(cps_data$FSWROUTY_binchar)
# handle missing values
cps_data <- na.omit(cps_data$FSWROUTY_binchar)
summary(cps_data)
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
gc()
rm(list=ls())
source("code/clean_cps.R")
# change y-variable into factor
cps_data$FSWROUTY_binchar <- ifelse(cps_data$FSWROUTY == 0, "No", "Yes")
# handle missing values
cps_data <- cps_data[!is.na(cps_data$FSWROUTY_binchar),]
summary(cps_data)
cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_bin))
cps_data <- cps_data %>% mutate(FSWROUTY_bin = as.factor(FSWROUTY_binchar))
summary(cps_data)
head(cps_data)
library(tidyverse)
library(knitr)
library(tibble)
library(ggthemes)
library(logistf)
library(glmnet)
library(haven)
library(pROC)
library(RColorBrewer)
library(randomForest)
library(ggplot2)
library(reshape2)
library(randomForest)
# set up
data_X <- cps_data[, c( "weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married")]
# standardize all predictor columns
data_stand <- apply(data_X, 2, function(x)(x - mean(x))/sd(x))
# compute observation-observation distance (for hierachical clustering)
data_dist <- dist(data_stand, method = "euclidean")
# first, let's use the average method to measure cluster-to-cluster similarity
data_clust <- hclust(data_dist, method = "average")
# the height of a bar is distance between cluster centers
plot(data_clust, labels = cps_data$FSWROUTY, hang = -1)
rect.hclust(data_clust, k = 3, border ="red")
# Making sense of the clusters, obtaining "characterization" of clusters
data_X$h_cluster <- as.factor(cutree(data_clust, k = 5))
data_X_long <- melt(data_X, id.vars = c("h_cluster"))
head(data_X_long)
ggplot(data = data_X_long) +
geom_boxplot(aes(x = h_cluster, y = value, fill = h_cluster)) +
facet_wrap(~variable, scales= "free") +
scale_fill_brewer("Cluster \nMembership", palette = "Dark2") +
ggtitle("Hierachical Clusters")
RNGkind(sample.kind = "default")
set.seed(159159)
train.idx <- sample(x=1:nrow(cps_data), size=.7*nrow(cps_data))
train.df <- cps_data[train.idx,]
test.df <- cps_data[-train.idx,]
test.df.preds <- test.df
rf_fswrouty <- randomForest(FSWROUTY ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "1"] #Choose positive event column
rf_fswrouty <- randomForest(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female ,
data = train.df,
ntree = 1000,
mtry = 4,
importance = TRUE)
pi_hat <- predict(rf_fswrouty, test.df, type = "prob")[, "Yes"]
rocCurve <- roc(response = test.df$FSWROUTY_bin,
predictor = pi_hat,
levels = c("No", "Yes"))
plot(rocCurve, print.thres = TRUE, print.ouc = TRUE)
plot(rocCurve, print.thres = TRUE, print.ouc = TRUE)
auc(rocCurve)
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'))
warnings()
beta <- fswrouty_mle %>% coef()
beta
fswrouty_mle <- glm(FSWROUTY_bin ~ hhsize + married + education + elderly +
kids + black + hispanic + female,
data = train.df,
family = binomial(link = 'logit'), weights = weight)
>>>>>>> Stashed changes
source("C:/Users/mattc/Desktop/FA24/STAT172Final/code/fsstamp_analysis.R")
str(reduced_train)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in 1:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
str(reduced_train)
reduced_train = train.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("weight", "hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:9){
for (j in i:9){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
str(reduced_test)
reduced_train = train.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:8){
for (j in i:8){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
reduced_train_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_train)[,-1]
reduced_test_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_test)[,-1]
reduced_train_y <- as.vector(reduced_train$FSSTMPVALC_bin)
reduced_test_y <- as.vector(reduced_test$FSSTMPVALC_bin)
################################################
# Making Lasso with Interaction/Squared Terms  #
################################################
lr_lasso_fsstmp_cv_2 <- cv.glmnet(reduced_train_matrix_x, reduced_train_y,
family=binomial(link="logit"), alpha=1, weights=train.weights)
best_lasso_lambda_fsstmp_2 <- lr_lasso_fsstmp_cv_2$lambda.min
lr_lasso_fsstmp_2 <- glmnet(reduced_train_x, reduced_train_y, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp_2, weights=train.weights)
lr_lasso_fsstmp_2 <- glmnet(reduced_train_matrix_x, reduced_train_y, family=binomial(link="logit"),
alpha=1, lambda = best_lasso_lambda_fsstmp_2, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_fsstmp_preds_2 = predict(lr_lasso_fsstmp_2, reduced_test_matrix_x, type="response")[,1]
)
lasso_fsstmp_rocCurve <- roc(response = as.factor(test.df.preds$FSSTMPVALC_bin),
predictor =test.df.preds$lasso_fsstmp_preds_2,
levels=c("0", "1"))
plot(lasso_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #Better at AUC = .798, (.681, .810)
lasso_fsstmp_pi_star <- coords(lasso_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
lr_lass_fsstmp_2
summary(lr_lasso_fsstmp_2)
str(reduced_train)
reduced_train_matrix_x
coef(lr_lasso_fsstmp_2)
reduced_train = train.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
reduced_test = test.df %>%
select(c("hhsize", "female", "hispanic", "black",
"kids", "elderly", "education", "married", "FSSTMPVALC_bin"))
FSSTMP.y.train <- as.vector(train.df$FSSTMPVALC_bin)
FSSTMP.y.test <- as.vector(test.df$FSSTMPVALC_bin)
interaction_df = data.frame(added_interaction=rep(NA, 81), AUC=rep(NA, 81))
inc = 1
for(i in 1:8){
for (j in i:8){
col1 = colnames(reduced_train)[i][1]
col2 = colnames(reduced_train)[j][1]
col_str = paste(col1, col2, sep="_")
reduced_train = reduced_train %>%
mutate(interaction_term = (reduced_train[col1] * reduced_train[col2])[,1])
reduced_test = reduced_test %>%
mutate(interaction_term = (reduced_test[col1] * reduced_test[col2])[,1])
names(reduced_train)[names(reduced_train) == "interaction_term"] = col_str
names(reduced_test)[names(reduced_test) == "interaction_term"] = col_str
}
}
reduced_train_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_train)[,-1]
reduced_test_matrix_x <- model.matrix(FSSTMPVALC_bin ~ .
, data=reduced_test)[,-1]
reduced_train_y <- as.vector(reduced_train$FSSTMPVALC_bin)
reduced_test_y <- as.vector(reduced_test$FSSTMPVALC_bin)
################################################
# Making Lasso with Interaction/Squared Terms  #
################################################
lr_lasso_fsstmp_cv_2 <- cv.glmnet(reduced_train_matrix_x, reduced_train_y,
family=binomial(link="logit"), alpha=0, weights=train.weights)
best_lasso_lambda_fsstmp_2 <- lr_lasso_fsstmp_cv_2$lambda.min
lr_lasso_fsstmp_2 <- glmnet(reduced_train_matrix_x, reduced_train_y, family=binomial(link="logit"),
alpha=0, lambda = best_lasso_lambda_fsstmp_2, weights=train.weights)
test.df.preds <- test.df.preds %>% mutate(
lasso_fsstmp_preds_2 = predict(lr_lasso_fsstmp_2, reduced_test_matrix_x, type="response")[,1]
)
lasso_fsstmp_rocCurve <- roc(response = as.factor(test.df.preds$FSSTMPVALC_bin),
predictor =test.df.preds$lasso_fsstmp_preds_2,
levels=c("0", "1"))
plot(lasso_fsstmp_rocCurve, print.thres=TRUE, print.auc=TRUE) #Better at AUC = .798, (.681, .810)
lasso_fsstmp_pi_star <- coords(lasso_fsstmp_rocCurve, "best", ref="threshold")$threshold[1]
warnings()
reduced_train_y
unique(reduced_train_y)
